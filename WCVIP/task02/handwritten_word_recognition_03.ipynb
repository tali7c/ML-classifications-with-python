{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwritten word recognition 03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIDtJf736QF8"
      },
      "source": [
        "Some time we have to install a package, if it is not present in goole colaboratory\n",
        "!pip install packageName"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OItDyQqm6sEA"
      },
      "source": [
        "import different required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9wJLT9y5lww"
      },
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "import _pickle as cPickle\n",
        "import pickle\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKUhtQBJ60wN"
      },
      "source": [
        "The dataset can be downloaded from \n",
        "https://drive.google.com/drive/folders/1ZiGxk6ZN5IBjtNAI4JkJK9YFehS8KNmT?usp=sharing\n",
        "\n",
        "create a directory and change the current directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4SxHFt0tBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6d2f78-15e3-4376-b072-46a8111392ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gdrive='/content/drive/'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d41jRMAWNHe0"
      },
      "source": [
        "change the myloc path according to your data folder\n",
        "\n",
        "Let create Training and Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_v8i6npDpBa"
      },
      "source": [
        "myloc='MyDrive/shared data/17 Dec 2020/task02/word_traintestset'\n",
        "rootDir=gdrive+myloc\n",
        "\n",
        "\n",
        "trainDir=rootDir+'/Train'\n",
        "trainList=[]\n",
        "count=0\n",
        "for path, subdirs, files in os.walk(trainDir):\n",
        "  for name in files:\n",
        "    if not name.endswith('.png'):\n",
        "      continue  \n",
        "    count = count +1\n",
        "    imPath=os.path.join(path, name); \n",
        "    image=cv2.imread(imPath,0) ### we want to read image as gray\n",
        "    h,w=image.shape[:2]\n",
        "    image=cv2.resize(image,(int(w*32/h),32))\n",
        "    print('Train ',count,image.shape)\n",
        "    parts=name.split('_');   \n",
        "    trainList.append([image, parts[0]])\n",
        "        \n",
        "      \n",
        "testDir=rootDir+'/Test'\n",
        "testList=[]\n",
        "count=0\n",
        "for path, subdirs, files in os.walk(testDir):\n",
        "  for name in files:\n",
        "    if not name.endswith('.png'):\n",
        "      continue         \n",
        "    count = count +1  \n",
        "    imPath=os.path.join(path, name); \n",
        "    image=cv2.imread(imPath,0) ### we want to read image as gray\n",
        "    h,w=image.shape[:2]\n",
        "    image=cv2.resize(image,(int(w*32/h),32))\n",
        "    print('Test',count,image.shape)\n",
        "    parts=name.split('_');   \n",
        "    testList.append([image, parts[0]])\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNalIaYZJFi"
      },
      "source": [
        "calculate some statistical information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9nGzb0sQGRB"
      },
      "source": [
        "\n",
        "maxWidth=0\n",
        "maxLen=0\n",
        "CharList=[]\n",
        "for im,word in trainList:\n",
        "  h,w=im.shape[:2]\n",
        "  if maxLen<len(word):\n",
        "    maxLen=len(word)\n",
        "  if maxWidth<w:\n",
        "    maxWidth=w\n",
        "  CharList = list(set(CharList + list(word)))\n",
        "\n",
        "\n",
        "for im,word in testList:\n",
        "  h,w=im.shape[:2]\n",
        "  if maxLen<len(word):\n",
        "    maxLen=len(word)\n",
        "  if maxWidth<w:\n",
        "    maxWidth=w\n",
        "  CharList = list(set(CharList + list(word)))\n",
        "\n",
        "with open(rootDir+'/DataSet.pkl', 'wb') as f:\n",
        "  cPickle.dump([trainList,testList,CharList,maxWidth,maxLen], f, pickle.HIGHEST_PROTOCOL)  "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4NLrmL2ZCx_"
      },
      "source": [
        "Load Data from saved pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM_8co5XbZod"
      },
      "source": [
        "with open(rootDir+'/DataSet.pkl', 'rb') as f:\n",
        "  trainList,testList,CharList,maxWidth,maxLen = cPickle.load(f)  \n",
        "\n",
        "\n",
        "class create_dataset(object):\n",
        "    def __init__(self,CharList,maxWidth,maxLen,batch_size):\n",
        "      self.CharList=CharList\n",
        "      self.maxWidth=maxWidth\n",
        "      self.maxLen=int(maxLen)\n",
        "      self.numChar=int(len(CharList))\n",
        "      self.batch_size=batch_size\n",
        "    def gen(self,dataList, phase='Train'):\n",
        "        inps=[]\n",
        "        labels=[]\n",
        "        label_length=[]\n",
        "        try:\n",
        "          while 1:\n",
        "              shuffle(dataList)\n",
        "              for img, word in dataList:\n",
        "                  # print(imPath)\n",
        "                  tmp=np.zeros((self.maxLen),dtype='int32')\n",
        "                  for cc,ch in enumerate(word):\n",
        "                    chi=self.CharList.index(ch)\n",
        "                    tmp[cc]=int(chi)\n",
        "                  labels.append(tmp.copy())\n",
        "                  label_length.append(len(word))\n",
        "\n",
        "                  tmpimage=np.zeros((32,self.maxWidth),dtype='uint8')             \n",
        "                  h,w=img.shape[:2]\n",
        "                  tmpimage[:h,:w]=img\n",
        "                  inps.append(np.expand_dims(tmpimage,axis=-1))\n",
        "                  if len(labels)==self.batch_size:            \n",
        "                      yield np.asarray(inps,dtype='float32'), np.asarray(labels,dtype='int32'), np.asarray(label_length,dtype='int32')\n",
        "                      inps=[]\n",
        "                      labels=[]\n",
        "                      label_length=[]\n",
        "              if phase=='Test':\n",
        "                  break\n",
        "        except GeneratorExit:\n",
        "          print(\"Generated Finished\")\n",
        "\n",
        "batch_size=32\n",
        "# print(len(trainList))\n",
        "dataset=create_dataset(CharList,maxWidth,maxLen,batch_size)\n",
        "traindata=dataset.gen(trainList)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCtkc_DGQtkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cc80e5b2-f6ae-40db-b5f6-b7059c456439"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "validdata=dataset.gen(testList,phase='Test')\n",
        "vimages, vclasses, label_length = next( validdata)\n",
        "\n",
        "# Fill out the subplots with the random images that you defined \n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(vimages[i][:,:,0])\n",
        "    plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()\n",
        "\n",
        " "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAACqCAYAAAAOaJyiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAFGUlEQVR4nO3bSWvcBRzH4d8sSZqlpnEpKNKg0arUehAXpHj0IIgXT14Ebx5cLhUKgiB6E9+CYFUoBBEED9KjonEpVO1BxbVitZDS1pY028zfdzBUTb+TpM9z/V++zI/5HGZpNU1TAGS0hz0A4GoiugBBogsQJLoAQaILECS6AEHdQQ9vn3+16f86Wf2xpvYeOl795eXUrm3paH++NewNl+PL32abC/0dtdyM1I7WWrVb/epUv871J+qF95+uuRc/G/bELWUr3P2xj59rLq2P1Ei7V1MjK7W0PlrfHZutuYOfV/lZ6b826OYDozuzc6lOz4zV3rk/qzqdjV/GprSrvVr3jHZqpNWvqqpe06+VZr1O9xar6XgDbkdvz71fnWrVctOrj5b21LneRF3cPzbsWdtSy58jAHJ8pgsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQ1B308M0fDjQPj/9ccyNTdeTCTB0+cG/1Fs+ktm07R/vzrWFvuBzfnry5+ejivnp06kTNdru12F+tU+vj9fwrz9bMW58Ne96WsxXuvvbnXHNyfamefOlgTb+zMOw5W96gmw+M7k3ds/VHb6qOr1xTR8/u2/hlbEqHzz5UO9pr9e3qjfXppfFa+HuuJrsrtTa56dvBf/TlSlPHlu+q9TE3vtIGRverpVurX62a7lyqtaZd1TSpXQzRM9d+Urva7XrjzIP11MxCPbnzZFVVPTT6wJCXcaX1xkX3Sms1QgoQ44s0gCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgCDRBQgSXYAg0QUIEl2AINEFCBJdgKDuoId3f/ByMzE/XdPvLqT2bGtH+/OtYW+4HK+deKzZ2VmuifZK/bS8u/aMnanzvYmaHV2sXrXrvdP31vLjq9U7d37YU7eErXD3Q18/0Rz55r664/Wl6p/4bthztrxBNx8Y3Yvnx2t1b7umN34Tm9jNo2dqcf2auq57sfbv+r1uaK/Uqd5EzXaXaqlp1bnrJ+rD1m3DnskG+uCX/TU2sVaruycHR4H/beDre+eev+rHU7OpLWwSk+3Vun/qRN3UbVWnWjXVnqpbRqqqpqqqaqF9aaj72Hij3fV65Jbv64vR+4c9ZdtrNU0z7A0AVw1fpAEEiS5AkOgCBIkuQJDoAgSJLkDQP8bu53k5W7EeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQUaI2slQyz3"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_iteration = 100\n",
        "display_step = 1\n",
        "\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "n_input = [28,28,1] # MNIST data input (img shape: 28*28)\n",
        "n_classes = dataset.numChar + 1 # MNIST total classes (0-9 digits)\n",
        "\n",
        "RNNnumKernelList=[128,64]\n",
        "scInitSize=np.sum(RNNnumKernelList)\n",
        "\n",
        "initializer=tf.keras.initializers.HeNormal()  \n",
        "# initializer=tf.keras.initializers.HeUniform()\n",
        "# initializer = tf.initializers.orthogonal(gain=1.0) \n",
        "# initializer = tf.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='truncated_normal')  \n",
        "# initializer = tf.initializers.glorot_uniform()  \n",
        "# initializer = tf.initializers.glorot_normal()  \n",
        "# initializer = tf.initializers.RandomUniform(minval=-1.0, maxval=1.0)  \n",
        "# initializer = tf.initializers.RandomNormal(mean=0, stddev=1.0)  \n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'cnn1': tf.Variable(initializer([3,3,n_input[2],32]),trainable=True),\n",
        "    'cnn2': tf.Variable(initializer([3,3,32,32]),trainable=True),\n",
        "    \n",
        "    \n",
        "    'cnn3': tf.Variable(initializer([3,3,32,64]),trainable=True),\n",
        "    'cnn4': tf.Variable(initializer([3,3,64,64]),trainable=True),    \n",
        "    \n",
        "    'cnn5': tf.Variable(initializer([3,3,64,64]),trainable=True),\n",
        "\n",
        "    'WString1':tf.Variable(initializer([4,RNNnumKernelList[0],RNNnumKernelList[0]]), trainable=True),\n",
        "    'WString2':tf.Variable(initializer([4,RNNnumKernelList[1],RNNnumKernelList[1]]), trainable=True),\n",
        "\n",
        "    'UString1':tf.Variable(initializer([4,64,RNNnumKernelList[0]]), trainable=True),\n",
        "    'UString2':tf.Variable(initializer([4,RNNnumKernelList[0],RNNnumKernelList[1]]), trainable=True),\n",
        "                       \n",
        "    'h1': tf.Variable(initializer([64*2, n_hidden_1]),trainable=True),\n",
        "    'h2': tf.Variable(initializer([n_hidden_1, n_hidden_2]),trainable=True),\n",
        "    'out': tf.Variable(initializer([n_hidden_2, n_classes]),trainable=True)\n",
        "}\n",
        "biases = {\n",
        "    'cb1': tf.Variable(tf.zeros([32]),trainable=True),\n",
        "    'cb2': tf.Variable(tf.zeros([32]),trainable=True),\n",
        "    'cb3': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "    'cb4': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "    'cb5': tf.Variable(tf.zeros([64]),trainable=True),\n",
        "\n",
        "    'BString1': tf.Variable(tf.zeros([4,RNNnumKernelList[0]]),trainable=True),\n",
        "    'BString2': tf.Variable(tf.zeros([4,RNNnumKernelList[1]]),trainable=True),\n",
        "    \n",
        "    'b1': tf.Variable(tf.zeros([n_hidden_1]),trainable=True),\n",
        "    'b2': tf.Variable(tf.zeros([n_hidden_2]),trainable=True),\n",
        "    'out': tf.Variable(tf.zeros([n_classes]),trainable=True)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR7nYmkfXZ0G"
      },
      "source": [
        "\n",
        "varList=[]\n",
        "for key in weights:\n",
        "  varList.append(weights[key])\n",
        "for key in biases:\n",
        "  varList.append(biases[key])\n",
        "\n",
        "\n",
        "def lstm1Cell(inp,ct_0,ht_0):\n",
        "  #  forget gate\n",
        "  fg = tf.sigmoid(tf.matmul(inp, weights['UString1'][0]) + tf.matmul(ht_0,weights['WString1'][0])+ biases['BString1'][0])\n",
        "  #  input gate\n",
        "  ig = tf.sigmoid(tf.matmul(inp, weights['UString1'][1]) + tf.matmul(ht_0,weights['WString1'][1]) + biases['BString1'][0])\n",
        "  \n",
        "  #  gate weights\n",
        "  ctg = tf.tanh(tf.matmul(inp, weights['UString1'][2]) + tf.matmul(ht_0,weights['WString1'][2]) + biases['BString1'][0])\n",
        "  \n",
        "  #  output gate\n",
        "  og = tf.sigmoid(tf.matmul(inp, weights['UString1'][3]) + tf.matmul(ht_0,weights['WString1'][3])+ biases['BString1'][0])\n",
        "  \n",
        "  ct_i = fg*ct_0 + ig*ctg\n",
        "  ht_i = og*tf.tanh(ct_i)\n",
        "  \n",
        "  return ct_i,ht_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lstm2Cell(inp,ct_0,ht_0):\n",
        "  #  forget gate\n",
        "  fg = tf.sigmoid(tf.matmul(inp, weights['UString2'][0]) + tf.matmul(ht_0,weights['WString2'][0])+ biases['BString2'][0])\n",
        "  #  input gate\n",
        "  ig = tf.sigmoid(tf.matmul(inp, weights['UString2'][1]) + tf.matmul(ht_0,weights['WString2'][1]) + biases['BString2'][0])\n",
        "  \n",
        "  #  gate weights\n",
        "  ctg = tf.tanh(tf.matmul(inp, weights['UString2'][2]) + tf.matmul(ht_0,weights['WString2'][2]) + biases['BString2'][0])\n",
        "  \n",
        "  #  output gate\n",
        "  og = tf.sigmoid(tf.matmul(inp, weights['UString2'][3]) + tf.matmul(ht_0,weights['WString2'][3])+ biases['BString2'][0])\n",
        "  \n",
        "  ct_i = fg*ct_0 + ig*ctg\n",
        "  ht_i = og*tf.tanh(ct_i)\n",
        "  \n",
        "  return ct_i,ht_i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def blsmCall(data):\n",
        "    data = tf.transpose(data,[1,0,2])\n",
        "    shape=tf.shape(data,out_type=tf.dtypes.int32)\n",
        "    # init_state_lstmString=tf.zeros((2,shape[1],self.scInitSize))\n",
        "    \n",
        "    # step - LSTM\n",
        "    def stepString(prev, x):\n",
        "      # gather previous internal state and output state\n",
        "     \n",
        "      # iterate through layers\n",
        "      ht, ct = [], []\n",
        "      inp = x\n",
        "      st=0\n",
        "      \n",
        "      ############### LSTM 1  ##################\n",
        "      ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[0]]),axis=0)\n",
        "      ct_i,ht_i=lstm1Cell(inp,ct_0,ht_0)   \n",
        "      inp = ht_i\n",
        "      ht.append(ht_i)\n",
        "      ct.append(ct_i)\n",
        "      st = st + RNNnumKernelList[0]\n",
        "\n",
        "\n",
        "      ############### LSTM 2  ##################\n",
        "      ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[1]]),axis=0)\n",
        "      ct_i,ht_i=lstm2Cell(inp,ct_0,ht_0)  \n",
        "      inp = ht_i\n",
        "      ht.append(ht_i)\n",
        "      ct.append(ct_i)\n",
        "      st = st + RNNnumKernelList[1]\n",
        "\n",
        "\n",
        "      return tf.stack([tf.concat(ct,axis=-1), tf.concat(ht,axis=-1)],axis=0)\n",
        "    \n",
        "    # tf.print('data ',tf.shape(data))\n",
        "    # tf.print('upflag ',tf.shape(upflag))\n",
        "    statesStringF = tf.scan(stepString, data, initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringF')\n",
        "    statesStringFct,statesStringFht=tf.unstack(statesStringF,axis=1)\n",
        "    statesStringFht=tf.split(statesStringFht,RNNnumKernelList,axis=-1)[-1]\n",
        "    \n",
        "    \n",
        "    statesStringB = tf.scan(stepString, tf.reverse(data,[0]), initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringB')\n",
        "    statesStringBct,statesStringBht=tf.unstack(statesStringB,axis=1)\n",
        "    statesStringBht=tf.split(statesStringBht,RNNnumKernelList,axis=-1)[-1]\n",
        "    statesStringBht= tf.reverse(statesStringBht,[0])\n",
        "    \n",
        "    rnnOut= tf.concat([statesStringFht,statesStringBht],axis=-1)\n",
        "    rnnOut = tf.transpose(rnnOut,[1,0,2])\n",
        "\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_1 = tf.add(tf.matmul(rnnOut, weights['h1']), biases['b1'])\n",
        "    layer_1=tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
        "    #drop1=tf.nn.dropout(layer_1,rate=0.25)\n",
        "    \n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2=tf.nn.leaky_relu(layer_2, alpha=0.2)\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    # out_layer=tf.nn.sigmoid(out_layer)\n",
        "    y=tf.nn.softmax(out_layer,axis=-1)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayerNN(x):\n",
        "\n",
        "    cnn1_out=tf.add( tf.nn.conv2d( x, weights['cnn1'], [1,1,1,1], padding='VALID'), biases['cb1'])\n",
        "    cnn1_actv=tf.nn.leaky_relu(cnn1_out, alpha=0.2)\n",
        "    \n",
        "    cnn2_out=tf.add( tf.nn.conv2d( cnn1_actv, weights['cnn2'], [1,1,1,1], padding='VALID'), biases['cb2'])\n",
        "    cnn2_actv=tf.nn.leaky_relu(cnn2_out, alpha=0.2)\n",
        "    \n",
        "    pool1=tf.nn.max_pool(cnn2_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
        "    \n",
        "    cnn3_out=tf.add( tf.nn.conv2d( pool1, weights['cnn3'], [1,1,1,1], padding='VALID'), biases['cb3'])\n",
        "    cnn3_actv=tf.nn.leaky_relu(cnn3_out, alpha=0.2)\n",
        "    cnn4_out=tf.add( tf.nn.conv2d( cnn3_actv, weights['cnn4'], [1,1,1,1], padding='VALID'), biases['cb4'])\n",
        "    cnn4_actv=tf.nn.leaky_relu(cnn4_out, alpha=0.2)\n",
        "    \n",
        "    pool2=tf.nn.max_pool(cnn4_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
        "    \n",
        "    cnn5_out=tf.add( tf.nn.conv2d( pool2, weights['cnn5'], [1,1,1,1], padding='VALID'), biases['cb5'])\n",
        "    cnn5_actv=tf.nn.leaky_relu(cnn5_out, alpha=0.2)\n",
        "    \n",
        "    rnnInp=tf.reduce_max(cnn5_actv,axis=1)\n",
        "\n",
        "\n",
        "    return rnnInp\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, rho=0.95, epsilon=10.0**-7)\n",
        "# optimizer = tf.keras.optimizers.SGD( learning_rate=learning_rate, momentum=0.0)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIWvtjtNXsE3"
      },
      "source": [
        "\n",
        "iteration=0\n",
        "while iteration<training_iteration:\n",
        "  iteration = iteration + 1 \n",
        "  images, classes, label_length = next(traindata)\n",
        "       \n",
        "  images=tf.convert_to_tensor(images)\n",
        "  images=(images-127.5)/127.5\n",
        "  classes=tf.convert_to_tensor(classes)\n",
        "  label_length=tf.convert_to_tensor(label_length)\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(varList)\n",
        "    rnnInp = multilayerNN(images)\n",
        "\n",
        "    y = blsmCall(rnnInp)\n",
        "\n",
        "    # data = tf.transpose(rnnInp,[1,0,2])\n",
        "    # shape=tf.shape(data,out_type=tf.dtypes.int32)\n",
        "    # # init_state_lstmString=tf.zeros((2,shape[1],self.scInitSize))\n",
        "    \n",
        "    # # step - LSTM\n",
        "    # def stepString(prev, x):\n",
        "    #   # gather previous internal state and output state\n",
        "     \n",
        "    #   # iterate through layers\n",
        "    #   ht, ct = [], []\n",
        "    #   inp = x\n",
        "    #   st=0\n",
        "      \n",
        "    #   ############### LSTM 1  ##################\n",
        "    #   ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[0]]),axis=0)\n",
        "    #   ct_i,ht_i=lstm1Cell(inp,ct_0,ht_0)   \n",
        "    #   inp = ht_i\n",
        "    #   ht.append(ht_i)\n",
        "    #   ct.append(ct_i)\n",
        "    #   st = st + RNNnumKernelList[0]\n",
        "\n",
        "\n",
        "    #   ############### LSTM 2  ##################\n",
        "    #   ct_0,ht_0=tf.unstack(tf.slice(prev,[0,0,st],[-1,-1,RNNnumKernelList[1]]),axis=0)\n",
        "    #   ct_i,ht_i=lstm2Cell(inp,ct_0,ht_0)  \n",
        "    #   inp = ht_i\n",
        "    #   ht.append(ht_i)\n",
        "    #   ct.append(ct_i)\n",
        "    #   st = st + RNNnumKernelList[1]\n",
        "\n",
        "\n",
        "    #   return tf.stack([tf.concat(ct,axis=-1), tf.concat(ht,axis=-1)],axis=0)\n",
        "    \n",
        "    # # tf.print('data ',tf.shape(data))\n",
        "    # # tf.print('upflag ',tf.shape(upflag))\n",
        "    # statesStringF = tf.scan(stepString, data, initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringF')\n",
        "    # statesStringFct,statesStringFht=tf.unstack(statesStringF,axis=1)\n",
        "    # statesStringFht=tf.split(statesStringFht,RNNnumKernelList,axis=-1)[-1]\n",
        "    \n",
        "    \n",
        "    # statesStringB = tf.scan(stepString, tf.reverse(data,[0]), initializer=tf.zeros((2,shape[1],scInitSize)),name='scanStringB')\n",
        "    # statesStringBct,statesStringBht=tf.unstack(statesStringB,axis=1)\n",
        "    # statesStringBht=tf.split(statesStringBht,RNNnumKernelList,axis=-1)[-1]\n",
        "    # statesStringBht= tf.reverse(statesStringBht,[0])\n",
        "    \n",
        "    # rnnOut= tf.concat([statesStringFht,statesStringBht],axis=-1)\n",
        "    # rnnOut = tf.transpose(rnnOut,[1,0,2])\n",
        "    # )\n",
        "    # # Hidden fully connected layer with 256 neurons\n",
        "    # layer_1 = tf.add(tf.matmul(rnnOut, weights['h1']), biases['b1'])\n",
        "    # layer_1=tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
        "    # #drop1=tf.nn.dropout(layer_1,rate=0.25)\n",
        "    \n",
        "    # # Hidden fully connected layer with 256 neurons\n",
        "    # layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    # layer_2=tf.nn.leaky_relu(layer_2, alpha=0.2)\n",
        "    # # Output fully connected layer with a neuron for each class\n",
        "    # out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    # # out_layer=tf.nn.sigmoid(out_layer)\n",
        "    # y=tf.nn.softmax(out_layer,axis=-1)\n",
        "\n",
        "\n",
        "    shape=tf.shape(y,out_type=tf.dtypes.int32)\n",
        "    loss = tf.reduce_mean(tf.nn.ctc_loss(classes, y, label_length, shape[1]*tf.ones_like(label_length), logits_time_major=False))\n",
        "    \n",
        "  gradients = tape.gradient(loss, varList)\n",
        "  optimizer.apply_gradients(zip(gradients, varList))\n",
        "\n",
        "  \n",
        "  # Display loss per epoch step\n",
        "  if iteration % display_step == 0:\n",
        "      print(\"iteration:\", '%04d' % (iteration), \"cost={:.9f}\".format(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ew3V-yAIHT5"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ6BeujKIPl-"
      },
      "source": [
        "\n",
        "with open(rootDir+'/Trainedweights.pkl', 'wb') as f:\n",
        "  cPickle.dump([weights,biases], f, pickle.HIGHEST_PROTOCOL)  \n",
        "\n",
        "\n",
        "print(len(testList))\n",
        "testdata=dataset.gen(testList,phase='Test')\n",
        "\n",
        "totalN=0\n",
        "totalP=0\n",
        "while True:\n",
        "  try:\n",
        "    images, classes, label_length = next(testdata)\n",
        "  except:\n",
        "    break\n",
        "  # print(classes)\n",
        "  images=tf.convert_to_tensor(images)\n",
        "  images=(images-127.5)/127.5\n",
        "  classes=tf.concat(tf.convert_to_tensor(classes),axis=0)\n",
        "  rnnInp = multilayerNN(images)\n",
        "  y = blsmCall(rnnInp)\n",
        "  y=tf.transpose(y,[1,0,2])\n",
        "  yencoded,log_probability=tf.nn.ctc_beam_search_decoder(y, label_length, beam_width=100, top_paths=1)\n",
        "\n",
        "  # print(len(yencoded))\n",
        "  # print(log_probability)\n",
        "  indices=yencoded[0].indices\n",
        "  values=tf.cast(yencoded[0].values,dtype='int32')\n",
        "  dense_shape=yencoded[0].dense_shape\n",
        "\n",
        "  indices0,indices1=tf.unstack(indices,axis=-1)\n",
        "  # print('indices ',indices)\n",
        "  print('indices0 ',indices0)\n",
        "  # print('indices1 ',indices1)\n",
        "  print('values ',values)\n",
        "  # print('dense_shape ',dense_shape)\n",
        "  \n",
        "  parts=tf.dynamic_partition( values, tf.cast(indices0,dtype='int32'), 32)\n",
        "  print(parts)\n",
        "\n",
        "  p=tf.reduce_sum(tf.cast( tf.equal(values,classes), dtype='float32'))\n",
        "  totalP = totalP + p\n",
        "  totalN = totalN + batch_size\n",
        "\n",
        "  if iteration % display_step == 0:\n",
        "      print(\"totalN:\", '%04d' % (totalN), \"totalP:\", '%04d' % (totalP), \"Accuracy={:.9f}\".format(totalP*100/totalN))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}